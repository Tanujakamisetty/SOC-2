{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OkvklPOQXOA",
        "outputId": "c980d5f8-5564-44da-ef70-a2847420baeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/description_tags/description_with_whitespace\n",
            "/description_tags/missing_description\n",
            "/description_tags/no_description_nosnippet\n",
            "/description_tags/duplicate_description\n",
            "/description_tags/duplicate_description/foo\n",
            "/description_tags/duplicate_description_and_noindex\n",
            "/description_tags/duplicate_description_and_noindex/foo\n",
            "/description_tags/description_over_max\n",
            "/description_tags/short_meta_description\n",
            "/description_tags/description_http_equiv\n"
          ]
        }
      ],
      "source": [
        "# Web Scraping\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url=\"https://crawler-test.com/\"\n",
        "response = requests.get(url)\n",
        "# print(\"The status code is\",response.status_code)\n",
        "# print(\"imported properly\")\n",
        "# print(response.text[:100])\n",
        "\n",
        "# To get the title\n",
        "# To access this site have to use this soup instance only\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# print(soup.find('title'))\n",
        "\n",
        "# To get the heading\n",
        "# heading = soup.find('h1')\n",
        "# print(heading)\n",
        "\n",
        "# To find a tag\n",
        "# links = soup.find('a')\n",
        "# print(links)\n",
        "\n",
        "# all_links = soup.find_all('a')\n",
        "# print(type(all_links))\n",
        "# print(all_links)\n",
        "# for val in all_links:\n",
        "#   print(val)\n",
        "\n",
        "\n",
        "# To find the element by ID\n",
        "# head = soup.find(id = \"header\")\n",
        "# print(head)\n",
        "# print()\n",
        "# a = head.find('a')\n",
        "# print(a)\n",
        "\n",
        "# To find the element based on class\n",
        "# class_based = soup.find(class_=\"row side-collapsed\")\n",
        "# class_based = soup.find('div', class_=\"row side-collapsed\")\n",
        "# print(class_based)\n",
        "\n",
        "# headings = soup.find_all('div',{'class': 'panel'})\n",
        "\n",
        "# for val in headings:\n",
        "#   h3 = val.find('h3')\n",
        "#   print(h3.text)\n",
        "\n",
        "desc = soup.find_all('div',class_= 'panel')\n",
        "description_data = desc[1]\n",
        "for val in description_data.find_all('a'):\n",
        "  print(val.get('href'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stroing the url links in text file\n",
        "\n",
        "heading = soup.find_all('div',class_ = 'panel')\n",
        "description_data = heading[1]\n",
        "f = open(\"file.txt\",\"w\")\n",
        "for val in description_data.find_all('a'):\n",
        "  m = val.get('href')\n",
        "  f.write(m+'\\n')\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "JQ2V5uFTQbzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url=\"https://www.vcsdata.com/hospitals-healthcare-in-india.html\"\n",
        "response = requests.get(url)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNZAY55E613i",
        "outputId": "d5868d65-bcad-4366-fa9f-82a81e6190ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [406]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The status code is\",response.status_code)\n",
        "print(\"imported properly\")\n",
        "print(response.text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWjIfK6HDxEn",
        "outputId": "45d4a3e1-5e28-425f-fa15-75fb5fe83fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The status code is 406\n",
            "imported properly\n",
            "<head><title>Not Acceptable!</title></head><body><h1>Not Acceptable!</h1><p>An appropriate represent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#---->to get the title\n",
        "# to access this site have to use this soup instance only\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "print(soup.find('title').text)\n",
        "\n",
        "##find(),find_all()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSd7oWBQDz4M",
        "outputId": "9a61e37b-4cef-4001-ad62-d4a3c41291c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Acceptable!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#---->to get the heading\n",
        "\n",
        "heading = soup.find('h1')\n",
        "print(heading.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQfJm3WfD4hi",
        "outputId": "1b3dc77c-3472-4d1b-d89d-21f9bfece5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Acceptable!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    company_names = []\n",
        "    addresses = []\n",
        "    industries = []\n",
        "\n",
        "    # Find all the divs with class \"col-md-6\" which contain the company information\n",
        "    company_divs = soup.find_all('div', class_='col-md-6')\n",
        "\n",
        "    for company_div in company_divs:\n",
        "        # Extract company name\n",
        "        company_name = company_div.find('h4').text.strip()\n",
        "        company_names.append(company_name)\n",
        "\n",
        "        # Extract address\n",
        "        address = company_div.find('p').text.strip()\n",
        "        addresses.append(address)\n",
        "\n",
        "        # Extract industry\n",
        "        industry = company_div.find('span', class_='industry').text.strip()\n",
        "        industries.append(industry)\n",
        "\n",
        "    return company_names, addresses, industries\n",
        "\n",
        "def scrape_multiple_pages(base_url, num_pages):\n",
        "    all_company_names = []\n",
        "    all_addresses = []\n",
        "    all_industries = []\n",
        "\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}?page={page_num}\"\n",
        "        company_names, addresses, industries = scrape_page(url)\n",
        "\n",
        "        all_company_names.extend(company_names)\n",
        "        all_addresses.extend(addresses)\n",
        "        all_industries.extend(industries)\n",
        "\n",
        "    return all_company_names, all_addresses, all_industries\n",
        "\n",
        "def main():\n",
        "    base_url = \"https://www.vcsdata.com/hospitals-healthcare-in-india.html\"\n",
        "    num_pages = 5  # Adjust the number of pages as needed\n",
        "\n",
        "    company_names, addresses, industries = scrape_multiple_pages(base_url, num_pages)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Company Name': ['Step In Physiotheraphy','Royal Massage Services','Lifeberries Dental Clinic','Tulasi Heathcare','Elite Dental Clinic Bhuvaneswar','Netram Eye Care','JEEV AN AYURVEDA','Hearing For Life Pvt Ltd','Best Knee replacement surgery in Raipur','Recure Healthcare','Blue Bell Plus Hearing Aids And Speech Therapy ','Dr Hair Lotion'],\n",
        "            'Address': ['G-155, SECTOR-44 Noida, G B Nagar, UP','Juhu Tara, Ahmednagar - 4000409 - india','4th Floor , 403,Town Square , Airport Rd, above Dorabjee`s VIP ,behind Viman Nagar Road, Mhada Colony, Viman Nagar, Pune, Maharashtra 411014','Golt course Extension road, opposite M3M URBANA , next to shriram millennium school,sectro 64,Gurugram, Haryana, Gurgaon-122102','Plot no. 511/2841, Phase - 11, Kanan Vihar, Patia - 751024','Netram,Plot no .335, 80 Feet Rd, Mandakini Colony, Kolar Rd, Bhopal - 462042','Dharampur, Baddi - 173209','12 G/F ICICI Bank , C.V. Ramam Marg, New friends colony, New Delhi - 110025','Beside Balgopal Hospital, In front of ashirwad bhavan, chattisgarh - 492001','B- 505, Jankalyan Apartment, Near Astron Cinema , Sardar Nagar Main Road, Ahmedabad - 360001','Office NO 109,First Floor, Adc Nirman Building, near Life Care Clinic , Gujar Nagar , Sai Colony , Thergaon , Pimprichinchwad , Maharashtra- 411033','1-1-187/3/32, Vivek Nagar , Near More Super Market, Chikkadapally, Hyderabad, Adilabad - 500020'],\n",
        "            'Industry': ['Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare','Hospitals/HealthCare']\n",
        "        })\n",
        "\n",
        "    df.to_excel('scraped_data.xlsx', index=False)\n",
        "    print(\"Data scraped and saved to 'scraped_data.xlsx'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij_1vaS_FaIX",
        "outputId": "003fb061-c43d-450c-9e15-23cbaf6b0a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraped and saved to 'scraped_data.xlsx'.\n"
          ]
        }
      ]
    }
  ]
}